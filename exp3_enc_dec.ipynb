{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "contest3_enc_dec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samsatp/NLP-contest3/blob/main/exp3_enc_dec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTGtpj3MIB41",
        "outputId": "2910e7e9-8593-47e6-f24b-16faceb17987"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NLP-contest3'...\n",
            "remote: Enumerating objects: 28, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 28 (delta 9), reused 20 (delta 6), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (28/28), done.\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1LGWy5VGVk6TXz_ZaYwGBvwJc5vFBzeZe\n",
            "To: /content/data.zip\n",
            "100% 24.1M/24.1M [00:01<00:00, 18.1MB/s]\n",
            "Archive:  /content/data.zip\n",
            "  inflating: dev_auto_tok.tsv        \n",
            "  inflating: dev_entities.json       \n",
            "  inflating: newly_tokenized/dev_auto_tok.tsv  \n",
            "  inflating: newly_tokenized/dev_entities.json  \n",
            "  inflating: newly_tokenized/train_auto_tok.tsv  \n",
            "  inflating: newly_tokenized/train_entities.json  \n",
            "  inflating: raw/dev_set.txt         \n",
            "  inflating: raw/test_set.txt        \n",
            "  inflating: train_auto_tok.tsv      \n",
            "  inflating: train_entities.json     \n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/samsatp/NLP-contest3.git\n",
        "\n",
        "!gdown 1LGWy5VGVk6TXz_ZaYwGBvwJc5vFBzeZe\n",
        "!unzip /content/data.zip\n",
        "!rm /content/data.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/result\n",
        "!mkdir /content/models"
      ],
      "metadata": {
        "id": "-ro0Yvn61_RD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib  \n",
        "preprocess = importlib.import_module(\"NLP-contest3.util.preprocess\")\n",
        "lst20utils = importlib.import_module(\"NLP-contest3.lst20utils\")\n",
        "\n",
        "maxlen = 45\n",
        "\n",
        "train_seqs = preprocess.get_sequence(\"/content/train_auto_tok.tsv\")\n",
        "train_seqs = [e for e in train_seqs if len(e) < maxlen]\n",
        "dev_seqs = preprocess.get_sequence(\"/content/dev_auto_tok.tsv\")\n",
        "dev_seqs = [e for e in dev_seqs if len(e) < maxlen]"
      ],
      "metadata": {
        "id": "OHUrOIHiIUht"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_raw = [\n",
        "                '|'.join([e[0] for e in row])\n",
        "                for row in train_seqs\n",
        "            ]\n",
        "\n",
        "Y_train_raw = [\n",
        "                [e[1] for e in row]\n",
        "                for row in train_seqs\n",
        "            ]\n",
        "\n",
        "X_dev_raw = [\n",
        "                '|'.join([e[0] for e in row]) \n",
        "                for row in dev_seqs\n",
        "            ]\n",
        "\n",
        "Y_dev_raw = [\n",
        "                [e[1] for e in row]\n",
        "                for row in dev_seqs\n",
        "            ]"
      ],
      "metadata": {
        "id": "VjaWQraaGzcX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tags = ['<BOS>', 'I_LOC', 'B_LOC', 'B_MEA', 'B_NUM', 'I_ORG', 'I_DES', 'I_MEA', 'B_TRM', 'B_DTM', 'B_DES', 'B_BRN', 'I_PER', 'I_NUM', 'B_ORG', 'I_DTM', 'I_BRN', 'B_PER', 'I_TRM', 'I_TTL', 'B_TTL']"
      ],
      "metadata": {
        "id": "x38vMJPOCHX6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_raw[0], Y_train_raw[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aq_MRDTK5CdQ",
        "outputId": "a6f16350-50af-47de-bc1e-28e965778057"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('ธรรมนูญ|แชมป์|สิงห์|คลาสสิก|กวาด|รางวัล|แสน|สี่|หมื่น|บาท',\n",
              " ['B_PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the appropriate sequence length to pad, truncate"
      ],
      "metadata": {
        "id": "Z_y08SKffgMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len_seqs = [ e.count(\"|\") for e in X_train_raw]\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.hist(len_seqs);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "P79guIvzNv1e",
        "outputId": "769580ff-82fc-4748-bf07-8c1b1cfe2657"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "print(np.mean(len_seqs))\n",
        "print(np.median(len_seqs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jiCdZ3sOAKW",
        "outputId": "f4eabb86-c007-4d8d-a986-b247a6499796"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21.81441410215147\n",
            "21.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data class"
      ],
      "metadata": {
        "id": "GnjtDwbF90cP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class SeqData:\n",
        "    def __init__(self, X, Y, maxlen = maxlen):\n",
        "        self.X = X \n",
        "        self.Y = Y\n",
        "        self.maxlen = maxlen\n",
        "        self.inp_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "                            filters='',\n",
        "                            lower=False,\n",
        "                            split='|',\n",
        "                            char_level=False,\n",
        "                            oov_token='<OOV>'\n",
        "                        )\n",
        "        self.tag_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "                            filters='',\n",
        "                            lower=False,\n",
        "                            oov_token='<OOV>'\n",
        "                        )\n",
        "        self.inp_tokenizer.fit_on_texts(X)\n",
        "        self.tag_tokenizer.fit_on_texts(Y)\n",
        "        self.tag_tokenizer.fit_on_texts([\"<BOS>\"])\n",
        "    \n",
        "    def padding(self, seqs, dtype='int32', value=0.0):\n",
        "        return tf.keras.preprocessing.sequence.pad_sequences(seqs, padding='post', truncating='post', maxlen=self.maxlen, dtype=dtype, value=value)\n",
        "\n",
        "\n",
        "    def get_bool_mask(self, seqs):\n",
        "        mask = [\n",
        "                [True for _ in seq] for seq in seqs\n",
        "            ]\n",
        "        return self.padding(mask, dtype=bool, value=False)\n",
        "\n",
        "    def preprocess(self, X, Y):\n",
        "        tokens = self.inp_tokenizer.texts_to_sequences(X)\n",
        "        tags   = self.tag_tokenizer.texts_to_sequences(Y)\n",
        "\n",
        "        tokens_pad =  self.padding(tokens)\n",
        "        tags_pad   =  self.padding(tags)\n",
        "\n",
        "        return tokens_pad, tags_pad\n",
        "\n",
        "    def load_dataset(self, X, Y, BATCH_SIZE=64):\n",
        "\n",
        "        # create boolean mask\n",
        "        bool_mask = self.get_bool_mask(Y)\n",
        "\n",
        "        # preprocess X\n",
        "        tokens, targets = self.preprocess(X, Y)\n",
        "\n",
        "        # create dataset\n",
        "        if BATCH_SIZE:\n",
        "            dataset = tf.data.Dataset.from_tensor_slices((tokens, targets, bool_mask))\\\n",
        "                        .shuffle(1000)\\\n",
        "                        .batch(BATCH_SIZE, drop_remainder=True)\n",
        "        else:\n",
        "            dataset = tf.data.Dataset.from_tensor_slices((tokens, targets, bool_mask))\\\n",
        "                        .shuffle(1000)\\\n",
        "                        .batch(len(X_dev_raw), drop_remainder=True)\n",
        "\n",
        "\n",
        "        return dataset, bool_mask, self.inp_tokenizer, self.tag_tokenizer"
      ],
      "metadata": {
        "id": "5Ha7YG5R2jB6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_creator = SeqData(X_train_raw, Y_train_raw)\n",
        "\n",
        "train_dataset, train_bool_mask, inp_tokenizer, tag_tokenizer = data_creator.load_dataset(X_train_raw, Y_train_raw)\n",
        "dev_dataset, dev_bool_mask, _, _ = data_creator.load_dataset(X_dev_raw, Y_dev_raw, BATCH_SIZE=None)"
      ],
      "metadata": {
        "id": "C5QjaaaF8C64"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_X_batch, example_Y_batch, example_mask = next(iter(train_dataset))\n",
        "print(example_X_batch.shape, example_Y_batch.shape, example_mask.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNtOLgDzBguN",
        "outputId": "c5c5c8cf-b494-4aa2-e3f9-532691319336"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 45) (64, 45) (64, 45)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_X_batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBuTqT9SJpos",
        "outputId": "89675976-2e0c-4c85-e6a7-061b82db109f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(64, 45), dtype=int32, numpy=\n",
              "array([[ 576,    2,    7, ...,    0,    0,    0],\n",
              "       [   5,   35,   13, ...,    0,    0,    0],\n",
              "       [   4,   10, 1020, ...,    0,    0,    0],\n",
              "       ...,\n",
              "       [  35,    3, 2045, ...,    0,    0,    0],\n",
              "       [ 208, 2334,  475, ...,    0,    0,    0],\n",
              "       [ 216,    2,  531, ...,    0,    0,    0]], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_Y_batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZNEBmJcCXVA",
        "outputId": "9cd76086-4707-4450-969e-13f77b19f83e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(64, 45), dtype=int32, numpy=\n",
              "array([[ 2,  2,  2, ...,  0,  0,  0],\n",
              "       [ 2,  2,  2, ...,  0,  0,  0],\n",
              "       [ 2,  2,  2, ...,  0,  0,  0],\n",
              "       ...,\n",
              "       [ 2,  2,  9, ...,  0,  0,  0],\n",
              "       [ 2,  4,  2, ...,  0,  0,  0],\n",
              "       [ 2,  2, 13, ...,  0,  0,  0]], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEtP6GR6ECSC",
        "outputId": "1ace27ba-0ef9-45d3-eb8b-b1564522ca47"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(64, 45), dtype=bool, numpy=\n",
              "array([[ True,  True,  True, ..., False, False, False],\n",
              "       [ True,  True,  True, ..., False, False, False],\n",
              "       [ True,  True,  True, ..., False, False, False],\n",
              "       ...,\n",
              "       [ True,  True,  True, ..., False, False, False],\n",
              "       [ True,  True,  True, ..., False, False, False],\n",
              "       [ True,  True,  True, ..., False, False, False]])>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. params"
      ],
      "metadata": {
        "id": "OTAjP_mm93vt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_inp_size = len(inp_tokenizer.word_index)+1\n",
        "vocab_tar_size = len(tag_tokenizer.word_index)+1\n",
        "max_length_input = example_X_batch.shape[1]\n",
        "max_length_output = example_Y_batch.shape[1]\n",
        "\n",
        "\n",
        "embedding_dim = 70\n",
        "enc_units = dec_units = 64\n",
        "enc_num_rnn_layers = dec_num_rnn_layers = 3"
      ],
      "metadata": {
        "id": "q595S90E4ue1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Encoder"
      ],
      "metadata": {
        "id": "aDWhes5m95wy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, num_layers, dropout=0):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        self.gru_layer = tf.keras.layers.RNN(\n",
        "            tf.keras.layers.StackedRNNCells([\n",
        "                tf.keras.layers.GRUCell(units = enc_units, dropout = dropout) \n",
        "                for _ in range(num_layers)\n",
        "            ]),\n",
        "            name = 'Encoder_RNN',\n",
        "            return_sequences = True,\n",
        "            return_state = True\n",
        "        )\n",
        "\n",
        "    def call(self, x, **kwr):\n",
        "        x = self.embedding(x)\n",
        "        output, *h = self.gru_layer(x, **kwr)\n",
        "        return output, h"
      ],
      "metadata": {
        "id": "zB91ibWB4KAg"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Test Encoder Stack\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, enc_units, enc_num_rnn_layers)\n",
        "\n",
        "# sample input\n",
        "sample_output, sample_h = encoder(example_X_batch)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder {} h vectors shape, each with shape: (batch size, units) {}'.format(enc_num_rnn_layers, sample_h[0].shape ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tRJd3Rf7NpR",
        "outputId": "8750be18-36a9-4661-ac3f-714cefa745ea"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 45, 64)\n",
            "Encoder 3 h vectors shape, each with shape: (batch size, units) (64, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UHeZ6WfTLBR",
        "outputId": "9379c47d-052a-4ea4-a904-07ab11922e71"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 45, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Decoder"
      ],
      "metadata": {
        "id": "N5I5waQ399nN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, num_layers, dropout=0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed = tf.keras.layers.Embedding(input_dim = vocab_size, output_dim = embedding_dim)\n",
        "        self.rnn   = tf.keras.layers.RNN(\n",
        "            tf.keras.layers.StackedRNNCells([\n",
        "                tf.keras.layers.GRUCell(units = dec_units, dropout = dropout) \n",
        "                for _ in range(num_layers)\n",
        "            ]),\n",
        "            name = 'Decoder_RNN',\n",
        "            return_sequences = True,\n",
        "            return_state = True\n",
        "        )\n",
        "        self.dense = tf.keras.layers.Dense(units = vocab_size)\n",
        "        \n",
        "    def init_state(self, enc_states):\n",
        "        return enc_states\n",
        "\n",
        "    def get_context(self, enc_output_sequences, X_shape):\n",
        "        context = enc_output_sequences[:,-1,:]\n",
        "        context_for_all_timestep = tf.repeat(tf.expand_dims(context, axis=1), repeats=X_shape[1], axis=1)\n",
        "        return context_for_all_timestep\n",
        "\n",
        "        \n",
        "    def call(self, X, enc_output_sequences, state, **kwargs):  ## X : (batch_size, max_steps) and state : (1, num_encRnn_hiddens)\n",
        "        \n",
        "        X = self.embed(X)  ## X : (batch_size, max_steps, emb_size)\n",
        "        ## construct Decoder input : (batch_size, max_steps, emb_size + last_state.shape[1])\n",
        "        context = self.get_context(enc_output_sequences, X.shape)\n",
        "        dec_input = tf.concat([X, context], axis=2)\n",
        "        \n",
        "        ## Forward pass: pass hidden state from rnn to Dense\n",
        "        rnn_output, *h = self.rnn(inputs=dec_input, initial_state=state)\n",
        "        \n",
        "        output = self.dense(rnn_output)   ## output : (batch_size, vocab_size)\n",
        "        \n",
        "        return output, h"
      ],
      "metadata": {
        "id": "6PkvkOkmNSM5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Model wrapper"
      ],
      "metadata": {
        "id": "1D7WJSRZgTdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(tf.keras.Model):\n",
        "    def __init__(self, encoder, decoder, tag_tokenizer, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.tag_tokenizer = tag_tokenizer\n",
        "    \n",
        "    def call(self, enc_inp, dec_inp, **kwargs):\n",
        "        enc_sequences, enc_states = self.encoder(enc_inp, **kwargs)\n",
        "        dec_init_state = self.decoder.init_state(enc_states)\n",
        "        \n",
        "        return self.decoder(dec_inp, enc_sequences, dec_init_state, **kwargs)"
      ],
      "metadata": {
        "id": "y1Vznt73gVa4"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, enc_units, enc_num_rnn_layers)\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, dec_units, dec_num_rnn_layers)\n",
        "\n",
        "enc_sequences, enc_states = encoder(example_X_batch)\n",
        "\n",
        "dec_init_state = decoder.init_state(enc_states)\n",
        "output, dec_state = decoder(example_Y_batch, enc_sequences, dec_init_state)"
      ],
      "metadata": {
        "id": "X05R-P83X-TQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output.shape, len(dec_state), dec_state[0].shape\n",
        "# next iter: output, dec_state = decoder(example_Y_batch, enc_sequences, dec_state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-I6bplRAaePq",
        "outputId": "7c604dd6-b896-4fe0-f037-1dfffa785fad"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 45, 26]), 3, TensorShape([64, 64]))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Loss function"
      ],
      "metadata": {
        "id": "ov9DYGQJAfZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedSoftmaxCELoss(tf.keras.losses.Loss):\n",
        "    \"\"\"The softmax cross-entropy loss with masks.\"\"\"\n",
        "    def __init__(self, bool_mask):\n",
        "        super().__init__(reduction='none')\n",
        "        self.bool_mask = tf.cast(bool_mask, tf.float32)\n",
        "\n",
        "    def mask_O_token(self, label):\n",
        "        O_index = tag_tokenizer.word_index[\"O\"]\n",
        "        return tf.cast(tf.where(label == O_index, False, True), tf.float32)\n",
        "\n",
        "    def call(self, label, pred):\n",
        "        #O_masked = self.mask_O_token(label)\n",
        "\n",
        "        label_one_hot = tf.one_hot(label, depth=pred.shape[-1])\n",
        "        unweighted_loss = tf.keras.losses.CategoricalCrossentropy(\n",
        "            from_logits=True, reduction='none')(label_one_hot, pred)\n",
        "        \n",
        "        weighted_loss = tf.reduce_mean((unweighted_loss*self.bool_mask), axis=1)\n",
        "        return weighted_loss"
      ],
      "metadata": {
        "id": "hmfJBKhCa2rT"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = MaskedSoftmaxCELoss(tf.constant([[True, True, False, False], [True, True, True, True], [False, False, False, False]]))\n",
        "test_label = tf.ones((3,4), dtype = tf.int32)\n",
        "test_pred = tf.ones((3, 4, 10))\n",
        "loss(test_label, test_pred).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFboDdXfcHUz",
        "outputId": "ff1c03a3-e38b-4d60-ca85-9190062307dc"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.1512926, 2.3025851, 0.       ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Train step"
      ],
      "metadata": {
        "id": "v4tNXzzdA_8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# example_X_batch, example_Y_batch, example_mask = next(iter(train_dataset))\n",
        "\n",
        "def train(encoder, decoder, dataset, lr, num_epochs):\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "    history = []\n",
        "    for epoch in range(num_epochs):\n",
        "        \n",
        "        losses = []\n",
        "        for i, (X, Y, mask) in enumerate(dataset):\n",
        "            ## X : (batch_size, maxlen)\n",
        "            ## Y : (batch_size, maxlen)\n",
        "\n",
        "            BOS_id = tag_tokenizer.word_index[\"<BOS>\"]\n",
        "            dec_input = tf.repeat( tf.expand_dims( [BOS_id], 1), X.shape[0], axis=0)\n",
        "            dec_input = tf.concat([dec_input, Y], axis=1)[:,:-1]\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                enc_seqs, enc_states = encoder(X)\n",
        "\n",
        "                dec_initial_state = decoder.init_state(enc_states)\n",
        "                output, dec_state = decoder(dec_input, enc_seqs, dec_initial_state)\n",
        "                \n",
        "                l = MaskedSoftmaxCELoss(mask)(Y, output)\n",
        "                losses.append(tf.reduce_mean(l).numpy())\n",
        "            \n",
        "            if i%100 == 0:\n",
        "                print(f'epoch {epoch} / batch: {i} / loss ', np.array(losses).mean())\n",
        "\n",
        "            variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "            gradients = tape.gradient(l, variables)\n",
        "            optimizer.apply_gradients(zip(gradients, variables))\n",
        "            \n",
        "\n",
        "        print(f'epoch {epoch} / loss ', np.array(losses).mean())\n",
        "        history.append(losses)\n",
        "    return history"
      ],
      "metadata": {
        "id": "Yra3TqK2cnjy"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Train the model"
      ],
      "metadata": {
        "id": "TjwL-HR7BK6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.list_physical_devices('GPU')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsOt0c30pLB7",
        "outputId": "88b76b94-f7c4-4154-c71b-d40922eec81d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, enc_units, enc_num_rnn_layers)\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, dec_units, dec_num_rnn_layers)"
      ],
      "metadata": {
        "id": "Vb2JRAnLeiof"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.005\n",
        "num_epochs = 10\n",
        "history = train(encoder, decoder, train_dataset, lr, num_epochs)"
      ],
      "metadata": {
        "id": "xnKue4uzeO-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(X):\n",
        "    enc_seqs, enc_states = encoder(X)\n",
        "    dec_state = decoder.init_state(enc_states)\n",
        "\n",
        "    BOS_id = tag_tokenizer.word_index[\"<BOS>\"]\n",
        "    dec_input = tf.repeat( tf.expand_dims( [BOS_id], 1), X.shape[0], axis=0)\n",
        "\n",
        "    for i in range(maxlen):\n",
        "        Y, dec_state = decoder(dec_input, enc_seqs, dec_state)\n",
        "\n",
        "        last_step_logit = Y[:,-1,:]\n",
        "        last_step_maxIndex = tf.expand_dims(tf.argmax(last_step_logit, -1, output_type=tf.int32), axis=1)\n",
        "        dec_input = tf.concat([dec_input, last_step_maxIndex], axis=1)\n",
        "\n",
        "    return dec_input.numpy()"
      ],
      "metadata": {
        "id": "44oewlCDjq3V"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test model's output"
      ],
      "metadata": {
        "id": "duI3b_sTCiw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i = 851\n",
        "X_sample, Y_sample = data_creator.preprocess(X_dev_raw[i:i+1], Y_dev_raw[i:i+1])\n",
        "output = inference(X_sample)\n",
        "\n",
        "print(Y_dev_raw[i:i+1])\n",
        "print(tag_tokenizer.sequences_to_texts(output))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qn7Hu4rBd5C5",
        "outputId": "6bc78002-1f4e-4e2f-e370-9b6b4a30bc0c"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['O', 'O', 'O', 'O', 'B_TTL', 'B_PER', 'I_PER', 'I_PER', 'I_PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B_ORG', 'I_ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B_DTM', 'I_DTM', 'I_DTM', 'I_DTM', 'I_DTM', 'I_DTM', 'I_DTM', 'I_DTM', 'I_DTM']]\n",
            "['<BOS> O O B_TTL B_PER I_PER I_PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save model"
      ],
      "metadata": {
        "id": "3lEsJF-Lcw88"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# wrap encoder and decoder to a single keras.Model\n",
        "model = Seq2Seq(encoder, decoder, tag_tokenizer)\n",
        "\n",
        "# pass data to build a graph\n",
        "_ = model(example_X_batch, example_Y_batch)\n",
        "\n",
        "# save model\n",
        "model.save(\"/content/model\")"
      ],
      "metadata": {
        "id": "u2kq_2qTAsie"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/model.zip /content/model"
      ],
      "metadata": {
        "id": "7qdRFXaeGVrr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea6d9ae7-ad1a-4af0-c376-343843298370"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/model/ (stored 0%)\n",
            "  adding: content/model/assets/ (stored 0%)\n",
            "  adding: content/model/variables/ (stored 0%)\n",
            "  adding: content/model/variables/variables.index (deflated 64%)\n",
            "  adding: content/model/variables/variables.data-00000-of-00001 (deflated 7%)\n",
            "  adding: content/model/keras_metadata.pb (deflated 92%)\n",
            "  adding: content/model/saved_model.pb (deflated 91%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# save tokenizers\n",
        "with open('/content/tag_tokenizer.pkl', 'wb') as outp:\n",
        "    pickle.dump(tag_tokenizer, outp, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open('/content/inp_tokenizer.pkl', 'wb') as outp:\n",
        "    pickle.dump(inp_tokenizer, outp, pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "srRDOftjC5jl"
      },
      "execution_count": 53,
      "outputs": []
    }
  ]
}